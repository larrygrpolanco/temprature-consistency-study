# =============================================================================
# TEMPERATURE CONSISTENCY STUDY - CONFIGURATION
# =============================================================================
# Edit this file to control what gets processed.
# Fine-grained control: specify exact temperatures, runs, and articles.

# -----------------------------------------------------------------------------
# DATASET SELECTION
# -----------------------------------------------------------------------------
# Options: 'validation' or 'test'
# - validation: 10 articles for testing pipeline before full run
# - test: 40 articles for actual data collection
dataset: test

# -----------------------------------------------------------------------------
# MODEL CONFIGURATION
# -----------------------------------------------------------------------------
# OpenAI model to use (must support temperature parameter)
model: gpt-4.1-mini

# Maximum tokens for completion (4096 is sufficient for longest articles)
max_tokens: 2048

# -----------------------------------------------------------------------------
# EXPERIMENT PARAMETERS
# -----------------------------------------------------------------------------
# Temperature values to test (must be floats)
# Study design: [0.0, 0.2, 0.4, 0.6, 0.8, 1.0, 1.3, 1.6, 2.0]
# For testing: use subset like [0.0, 0.3]
temperatures: [0.0, 0.2, 0.4, 0.6, 0.8, 1.0, 1.3, 1.6, 2.0]


# Run numbers (for consistency evaluation across multiple iterations)
# Study design: 1-50 for each temperature
# For testing: use subset like [1, 2, 3]
# For re-runs: specify exact runs like [5, 12, 23]
runs: [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50]


# Article selection
# Options:
#   'all' - process all articles in the dataset
#   List of specific articles - ['test001', 'test005', 'test012']
# For re-runs: specify only articles that need re-processing
articles: 'all'

# Force rerun of completed work
# Set to true when you need to rerun articles even if outputs already exist
# (e.g., after validation failures or to regenerate outputs)
force_rerun: false

# -----------------------------------------------------------------------------
# PARSING & VALIDATION
# -----------------------------------------------------------------------------
# Text similarity threshold for validation (0.0 to 1.0)
# Lower = stricter matching, higher = more lenient
# Recommended: 0 (similarity not measured, all sentence variations accepted)
text_similarity_threshold: 0

# -----------------------------------------------------------------------------
# API CONFIGURATION
# -----------------------------------------------------------------------------
# API key should be set as environment variable: OPENAI_API_KEY
# Do not hardcode API keys in this file
api_key_env: OPENAI_API_KEY

# -----------------------------------------------------------------------------
# NOTES FOR REPLICATION
# -----------------------------------------------------------------------------
# To replicate the full study:
# 1. Set dataset: test
# 2. Set temperatures: [0.0, 0.2, 0.4, 0.6, 0.8, 1.0, 1.3, 1.6, 2.0]
# 3. Set runs: list from 1 to 50
# 4. Set articles: all
# 5. Run scripts in order: 01 → 02 → 03 → 04 → 05
#
# Estimated costs (based on GPT-4.1-mini pricing, November 2025):
# - Full experiment: 18,000 API calls = ~$40
# - Validation test (5 temps × 5 runs × 10 articles): ~$0.55
# - Cost per API call: ~$0.0022
#
# Estimated runtime (on standard laptop):
# - Full experiment: ~8 hours
# - Test run: ~20 minutes
