# Temperature and Consistency in LLM-Based Move-Step Annotation

## Overview

This replication package contains code and data for reproducing the results of a study examining the effect of temperature settings on the accuracy and consistency of Large Language Model (LLM) annotations for rhetorical move-step analysis.

The study investigates how different temperature parameters affect:

- **Accuracy**: Agreement between LLM predictions and expert gold-standard annotations
- **Consistency**: Agreement between multiple runs at the same temperature (measured using Krippendorff's alpha)

## Data Availability

This study uses the **CaRS-50 corpus**:

- **Source:** Lam, Charles; Nnamoko, Nonso (2025), "CaRS-50 Dataset: Annotated corpus of rhetorical Moves and Steps in 50 article introductions", Mendeley Data, V1
- **DOI:** [10.17632/kwr9s5c4nk.1](https://doi.org/10.17632/kwr9s5c4nk.1)
- **License:** CC BY 4.0
- **Location in package:** `data/raw_xml/`

The corpus contains 50 Biology research article introductions (1,297 sentences) with expert move-step annotations following the CaRS (Create a Research Space) framework.

**Note:** The XML files are not included in this repository. Please download them from Mendeley Data and place them in the `data/raw_xml/` directory before running the scripts.

## Requirements

### Software

- Python 3.9 or higher
- OpenAI API access (GPT-4 or GPT-5)

### Python Packages

Install dependencies:

```bash
pip install -r requirements.txt
```

Key packages:

- `openai` (≥1.12.0) - OpenAI API client
- `pandas` (≥2.0.0) - Data manipulation
- `numpy` (≥1.24.0) - Numerical computing
- `krippendorff` (≥0.6.0) - Inter-rater reliability
- `pyyaml` (≥6.0) - Configuration management
- `matplotlib` (≥3.7.0) - Plotting
- `seaborn` (≥0.12.0) - Statistical visualizations
- `scikit-learn` (≥1.3.0) - Machine learning utilities
- `python-dotenv` (≥1.0.0) - Environment variable management
- `scipy` - Statistical functions

### API Configuration

Set your OpenAI API key as an environment variable:

```bash
export OPENAI_API_KEY='your-key-here'
```

Or create a `.env` file in the project root:

```
OPENAI_API_KEY=your-key-here
```

## Project Structure

```
temperature_consistency_study/
│
├── README.md                           # This file
├── requirements.txt                    # Python dependencies
├── config.yaml                         # Experimental configuration
│
├── data/
│   ├── raw_xml/                       # Original CaRS-50 XML files (download separately)
│   │   ├── text001.xml
│   │   ├── text002.xml
│   │   └── ... (text001-050.xml)
│   │
│   └── processed/                      # Generated by script 01
│       ├── validation/
│       │   ├── gold.json              # Gold standard annotations
│       │   └── input/                 # Plain text for API calls
│       │       ├── validation001.txt
│       │       └── ... (10 files)
│       │
│       ├── test/
│       │   ├── gold.json
│       │   └── input/
│       │       ├── test001.txt
│       │       └── ... (40 files)
│       │
│       └── split_report.txt           # Stratification documentation
│
├── prompts/
│   └── system_prompt.txt              # LLM annotation instructions
│
├── outputs/                            # Generated by scripts 02 & 03
│   ├── temp_0.0/
│   │   ├── run_01/
│   │   │   ├── raw/                   # Raw LLM outputs
│   │   │   │   ├── test001.txt
│   │   │   │   └── ... (40 files)
│   │   │   └── parsed.json            # All articles combined
│   │   ├── run_02/
│   │   └── ... (run_01 through run_50)
│   │
│   ├── temp_0.3/
│   ├── temp_0.7/
│   ├── temp_1.0/
│   └── temp_1.5/
│
├── results/                            # Generated by scripts 03, 04, 05
│   ├── validation_reports/
│   │   ├── temp_0.0_run_01_test001.txt
│   │   └── ...
│   │
│   ├── metrics/
│   │   ├── accuracy_by_temperature.json
│   │   ├── krippendorff_alpha.json
│   │   └── sentence_level_analysis.json
│   │
│   └── figures/
│       ├── figure_1_accuracy_consistency.png
│       ├── figure_2_step_level.png
│       ├── figure_3_sentence_consistency.png
│       ├── table_2_accuracy.csv
│       └── table_4_consistency.csv
│
└── scripts/
    ├── 01_prepare_dataset.py          # XML → gold.json + stratified split
    ├── 02_run_api.py                  # LLM API calls → raw outputs
    ├── 03_parse_validate.py           # Parse raw → parsed.json + validate
    ├── 04_calculate_metrics.py        # Compute accuracy & consistency
    └── 05_generate_figures.py         # Create visualizations
```

## Runtime and Cost Estimates

### Full Experiment

- **API calls:** 18,000 (9 temperatures × 50 runs × 40 articles)
- **Estimated time:** 8-10 hours per temperature condition (sequential); total time depends on parallelization strategy
- **Estimated cost:** ~$40 USD (based on GPT-4.1-mini pricing, November 2025: ~$0.0022 per call)

**Note:** Actual costs depend on current OpenAI pricing and may vary. Runtime can be reduced significantly by running multiple temperature conditions in parallel (see configuration options in `config.yaml`). Always test with the validation set first!

## Reproduction Steps

### 1. Download CaRS-50 Data

Download the CaRS-50 corpus XML files from [Mendeley Data](https://doi.org/10.17632/kwr9s5c4nk.1) and place them in `data/raw_xml/`.

### 2. Prepare Dataset

Convert XML files into structured format and create stratified validation/test split:

```bash
python scripts/01_prepare_dataset.py
```

**Output:**

- Gold standard JSON files with annotations
- Plain text input files for API calls
- Split report documenting stratification

### 3. Configure Experiment

Edit `config.yaml` to control:

- Dataset selection (`validation` for testing, `test` for full experiment)
- Model selection (e.g., `gpt-4-turbo-preview`, `gpt-5-2025-08-07`)
- Temperature values to test
- Number of runs per temperature
- Articles to process

**For initial testing:**

```yaml
dataset: validation
temperatures: [0.0, 0.3]
runs: [1, 2, 3]
articles: all
```

**For full replication:**

```yaml
dataset: test
temperatures: [0.0, 0.3, 0.7, 1.0, 1.5]
runs: [1, 2, 3, ..., 50] # List all runs 1-50
articles: all
```

### 4. Run API Calls

Call OpenAI API and save raw outputs:

```bash
python scripts/02_run_api.py
```

**Features:**

- Automatically checks for existing outputs to avoid duplicate API calls
- Provides progress updates for each temperature and run
- Saves raw LLM responses for later parsing

**Tip:** You can run this script multiple times with different configurations (e.g., different temperature subsets) to parallelize the work.

### 5. Parse and Validate

Parse raw outputs, validate against gold standard, and create combined JSON files:

```bash
python scripts/03_parse_validate.py
```

**Validation checks:**

- Sentence count matches gold standard
- Text similarity above threshold (configurable in `config.yaml`)

**If validation fails:**

- Review reports in `results/validation_reports/`
- Update `config.yaml` to re-run specific articles
- Re-run scripts 02 and 03

### 6. Calculate Metrics

Compute accuracy, consistency, and Krippendorff's alpha:

```bash
python scripts/04_calculate_metrics.py
```

**Metrics calculated:**

- Move-level and step-level accuracy
- Krippendorff's alpha for inter-run agreement
- Sentence-level consistency analysis

### 7. Generate Figures

Create publication-ready figures and tables:

```bash
python scripts/05_generate_figures.py
```

**Output files:**

- `figure_1_accuracy_consistency.png` - Dual-axis plot of accuracy and alpha
- `figure_2_step_level.png` - Step-level accuracy across temperatures
- `figure_3_sentence_consistency.png` - Distribution of sentence consistency
- `table_2_accuracy.csv` - Accuracy summary table
- `table_4_consistency.csv` - Consistency (alpha) summary table

## Configuration Details

### Temperature Settings

The study design uses five temperature values:

- **0.0** - Deterministic (as close as possible)
- **0.3** - Low temperature
- **0.7** - Medium temperature
- **1.0** - High temperature (default)
- **1.5** - Very high temperature

### Text Similarity Threshold

The validation script uses a text similarity threshold (default: 0.85) to verify that LLM outputs preserve the original sentence text. This allows for minor variations (punctuation, spacing) while flagging substantial changes.

### Stratified Split

The dataset is split 20% validation / 80% test using stratified sampling to ensure move and step distributions are balanced across splits.

## Notes for Replicators

### Random Variation

LLM outputs are stochastic. Even with `temperature=0.0`, minor differences (<1%) in final metrics are expected across replications due to API-level randomness.

### Validation Set

Always test the pipeline with the validation set (10 articles) before running the full experiment. This helps catch configuration issues early and provides cost estimates.

### Partial Re-runs

If some API calls fail or validation errors occur, you can re-run specific articles/runs by editing `config.yaml`:

```yaml
temperatures: [0.3] # Only re-run temperature 0.3
runs: [5, 12, 23] # Only re-run these specific runs
articles: ['test012', 'test015'] # Only these articles
```

### Script Execution Order

Scripts must be run in order (01 → 02 → 03 → 04 → 05) since each depends on outputs from the previous one.

## Output Files

All figures and tables use descriptive names:

- **Figure 1:** `figure_1_accuracy_consistency.png` - Main results showing accuracy and consistency trends
- **Figure 2:** `figure_2_step_level.png` - Detailed step-level breakdown
- **Figure 3:** `figure_3_sentence_consistency.png` - Sentence consistency distribution
- **Table 2:** `table_2_accuracy.csv` - Move-level accuracy statistics
- **Table 4:** `table_4_consistency.csv` - Krippendorff's alpha values

## Troubleshooting

### API Key Issues

If you see "OPENAI_API_KEY environment variable not set":

```bash
export OPENAI_API_KEY='your-key-here'
```

### Missing XML Files

If script 01 reports "No XML files found":

- Download CaRS-50 from [Mendeley Data](https://doi.org/10.17632/kwr9s5c4nk.1)
- Extract files to `data/raw_xml/`
- Ensure files are named `text001.xml` through `text050.xml`

### Validation Failures

If script 03 reports validation failures:

1. Check `results/validation_reports/` for details
2. Common issues:
   - LLM missed or added sentences (sentence count mismatch)
   - LLM modified sentence text (low similarity score)
3. Re-run specific failed articles using script 02

### Metrics Not Found

If script 05 reports "No metrics found":

- Ensure script 04 completed successfully
- Check that `results/metrics/*.json` files exist

## License

- **Code:** MIT License
- **CaRS-50 Data:** CC BY 4.0

## Citation

If you use this replication package, please cite:

```
[Your citation information here]
```

And cite the CaRS-50 corpus:

```
Lam, Charles; Nnamoko, Nonso (2025), "CaRS-50 Dataset: Annotated corpus of
rhetorical Moves and Steps in 50 article introductions", Mendeley Data, V1,
doi: 10.17632/kwr9s5c4nk.1
```

## Contact

For questions or issues with this replication package, please open an issue on GitHub or contact:

[Your contact information]

## Acknowledgments

This study uses the CaRS (Create a Research Space) framework originally developed by Swales (1990, 2004) for analyzing research article introductions.
